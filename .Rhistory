filter(channel != "BBCNEWS") %>%
mutate(ranking = if_else(channel == "FOXNEWSW",
1, if_else(channel == "CNNW", 2, 3)))
chyrons <- read_csv(chyron_data.csv)
chyrons <- read_csv("chyron_data.csv")
nrow(chyrons)
glimpse(chyrons)
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
glimpse(chyrons)
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hm(date_time_utc)) %>%
mutate(date_x = as.Date(substr(date, 1, 10)),
hour = as.double(substr(date, 12, 13))) %>%
select(channel, text, ranking, date_x, hour) %>%
mutate(post_election = ifelse(date_x > "2020-11-03", "Post-Election", "Pre-Election"),
primetime = ifelse(hour > 19 & hour < 24, "Primetime", "Not Primetime"))
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hm(date_time_utc)) %>%
mutate(date_x = as.Date(substr(date, 1, 10)),
hour = as.double(substr(date, 12, 13))) %>%
select(channel, text, date_x, hour) %>%
mutate(post_election = ifelse(date_x > "2020-11-03", "Post-Election", "Pre-Election"),
primetime = ifelse(hour > 19 & hour < 24, "Primetime", "Not Primetime"))
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hm(date_time_utc)) %>%
mutate(date_x = as.Date(substr(date, 1, 10)),
hour = as.double(substr(date, 12, 13))) %>%
select(channel, text, date_x, hour)
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hm(date_time_utc)) %>%
mutate(date_x = as.Date(substr(date, 1, 10)),
hour = as.double(substr(date, 12, 13))) %>%
select(channel, text, date_x, hour)
tidy_chyrons
View(chyrons)
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hm(date_time_utc))
?mdy_hms
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hms(date_time_utc))
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hm(date_time_utc))
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
tidy_chyrons <- chyrons %>%
mutate(date = mdy_hms(date_time_utc))
c <- read_tsv("third-eye.tsv")
c
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10))
tidy_chyrons
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub))
tidy_chyrons
tidy_chyrons
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub)
tidy_chyrons
text_corpus <- corpus(tidy_chyrons, text_field = "text")
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub)
# Turning data into a corpus for quanteda
text_corpus <- corpus(tidy_chyrons, text_field = "text")
glimpse(tidy_chyrons)
# reading in data
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
# cleaning data, changing the date formats
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub)
# Turning data into a corpus for quanteda
text_corpus <- corpus(tidy_chyrons, text_field = "text")
library(haven)
knitr::opts_chunk$set(echo = TRUE)
# Loading necessary packages
library(gt)
library(quanteda)
library(broom)
library(skimr)
library(lubridate)
library(janitor)
library(dotwhisker)
library(tidytext)
library(haven)
library(ggthemes)
library(webshot)
library(stargazer)
library(tidyverse)
library(patchwork)
library(ggrepel)
library(ggpubr)
congress <- read_sav("congress.sav")
glimpse(congress)
knitr::opts_chunk$set(echo = TRUE)
# Loading necessary packages
library(gt)
library(quanteda)
library(broom)
library(skimr)
library(lubridate)
library(janitor)
library(dotwhisker)
library(tidytext)
library(haven)
library(ggthemes)
library(webshot)
library(stargazer)
library(tidyverse)
library(patchwork)
library(ggrepel)
library(ggpubr)
# reading in data
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
# reading in data
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
# cleaning data, changing the date formats
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub)
# Turning data into a corpus for quanteda
text_corpus <- corpus(tidy_chyrons, text_field = "text")
glimpse(tidy_chyrons)
# cleaning data, changing the date formats
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
# cleaning data, changing the date formats
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
tidy_chyrons
text_corpus <- corpus(tidy_chyrons, text_field = "text")
text_corpus
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details) %>%
drop_na()
text_corpus <- corpus(tidy_chyrons, text_field = "text")
text_corpus
summary(tidy_chyrons)
View(tidy_chyrons)
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
summary(tidy_chyrons)
# reading in data
chyrons <- read_csv("chyron_data.csv") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
# cleaning data, changing the date formats. Need to decide what to do with the duration thing
tidy_chyrons <- chyrons %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
summary(tidy_chyrons)
# Turning data into a corpus for quanteda
text_corpus <- corpus(tidy_chyrons, text_field = "text")
summary(tidy_chyrons)
read.csv("third-eye.tsv", sep = "\t")
d <- read.csv("third-eye.tsv", sep = "\t")
d <- read.csv("third-eye.tsv", sep = "\t",
quote = "")
d <- read.csv("third-eye.tsv", sep = "\t",
quote = "")
summary(d)
tidy_d <- d %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
d <- read.csv("third-eye.tsv", sep = "\t",
quote = "") %>%
clean_names() %>%
filter(channel != "BBCNEWS")
tidy_d <- d %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
summary(d)
tidy_d
summary(d$date)
tidy_d <- d %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
tidy_d
summary(d)
summary(tidy_chyrons)
random_numbers <- sample(1:217, replace = FALSE)
random_numbers
random_numbers <- sample(1:217, 10, replace = FALSE)
random_numbers
set.seed(1121)
random_numbers <- sample(1:217, 10, replace = FALSE)
random_numbers
data_1 <- read_csv("data/third_eye_01_01_21.csv")
data_2 <- read_csv("data/third_eye_01_26_28.csv")
data_3 <- read_csv("data/third_eye_03_29_19.csv")
data_4 <- read_csv("data/third_eye_05_31_19.csv")
data_5 <- read_csv("data/third_eye_06_28_19.csv")
data_6 <- read_csv("data/third_eye_07_10_20.csv")
data_7 <- read_csv("data/third_eye_06_29_18.csv")
data_8 <- read_csv("data/third_eye_09_27_19.csv")
?bind_rows
data_joined <- data_1 %>%
bind_rows(data_2,
data_3,
data_4)
data_1 <- read_csv("data/third_eye_01_01_21.csv")
data_2 <- read_csv("data/third_eye_01_26_28.csv")
data_3 <- read_csv("data/third_eye_03_29_19.csv")
data_4 <- read_csv("data/third_eye_05_31_19.csv")
data_5 <- read_csv("data/third_eye_06_28_19.csv")
data_6 <- read_csv("data/third_eye_07_10_20.csv")
data_7 <- read_csv("data/third_eye_06_29_18.csv")
data_8 <- read_csv("data/third_eye_09_27_19.csv")
data_joined <- data_1 %>%
bind_rows(data_2,
data_3,
data_4)
data_2
data_2 <- read_csv("data/third_eye_01_26_28.csv")
data_1
data_1 <- read_csv("data/third_eye_01_01_21.csv")
data_2 <- read_csv("data/third_eye_01_26_18.csv")
data_3 <- read_csv("data/third_eye_03_29_19.csv")
data_4 <- read_csv("data/third_eye_05_31_19.csv")
data_5 <- read_csv("data/third_eye_06_28_19.csv")
data_6 <- read_csv("data/third_eye_07_10_20.csv")
data_7 <- read_csv("data/third_eye_06_29_18.csv")
data_8 <- read_csv("data/third_eye_09_27_19.csv")
data_joined <- data_1 %>%
bind_rows(data_2,
data_3,
data_4)
data_joined
data_joined <- data_1 %>%
bind_rows(data_2,
data_3,
data_4,
data_5,
data_6,
data_7,
data_8)
summary(data_joined)
tidy_data <- data_joined %>%
clean_names() %>%
filter(channel != "BBCNEWS") %>%
mutate(date_sub = substr(date_time_utc, 1, 10),
date = ymd(date_sub)) %>%
select(-date_sub, - date_time_utc, -https_archive_org_details)
summary(tidy_data)
View(tidy_data)
?write_csv
write_csv(tidy_data, "tidy_chyron_data.csv")
tidy_chyrons <- read_csv("tidy_chyron_data.csv")
tidy_chyrons <- read_csv("tidy_chyron_data.csv")
text_corpus <- corpus(tidy_chyrons, text_field = "text")
toks <- tokens(text_corpus,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern=stopwords("en")) %>%
tokens_remove(pattern = c("u201c", "u00b0", "u2014", "wopi", "avi", "ooo",
"000", "ito", "ynl", "f'avl", "foxnews.com",
"ufb02pi", "ufb021", "ufbo2l", "ufb02L", "rpm")) %>%
tokens_select(min_nchar = 3) %>%
tokens_ngrams(n = 2)
wordcloud_dfm <- dfm(toks, groups = "channel")
textplot_wordcloud(wordcloud_dfm, comparison = T,
min_count = 150)
textplot_wordcloud(wordcloud_dfm, comparison = T,
min_count = 200)
textplot_wordcloud(wordcloud_dfm, comparison = T,
min_count = 200,
min_size = .5,
max_size = 3.7)
# turning corpus into tokens. Removing unimportant words and selectng for ngrams
# = 2. Code taken from gov51 final project
toks_2 <- tokens(text_corpus,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern=stopwords("en")) %>%
tokens_remove(pattern = c("u201c", "u00b0", "u2014", "wopi", "avi", "ooo",
"000", "ito", "ynl", "f'avl", "foxnews.com",
"ufb02pi", "ufb021", "ufbo2l", "ufb02L", "rpm")) %>%
tokens_select(min_nchar = 3) %>%
tokens_ngrams(n = 2)
wordcloud_dfm_2 <- dfm(toks_2, groups = "channel")
textplot_wordcloud(wordcloud_dfm_2, comparison = T,
min_count = 200,
min_size = .5,
max_size = 3.7)
# turning corpus into tokens. Removing unimportant words and selectng for ngrams
# = 2. Code taken from gov51 final project
toks_3 <- tokens(text_corpus,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern=stopwords("en")) %>%
tokens_remove(pattern = c("u201c", "u00b0", "u2014", "wopi", "avi", "ooo",
"000", "ito", "ynl", "f'avl", "foxnews.com",
"ufb02pi", "ufb021", "ufbo2l", "ufb02L", "rpm")) %>%
tokens_select(min_nchar = 3) %>%
tokens_ngrams(n = 3)
# thing to talk about -- Mueller report
wordcloud_dfm_3 <- dfm(toks_3, groups = "channel")
textplot_wordcloud(wordcloud_dfm_3, comparison = T,
min_count = 200,
min_size = .5,
max_size = 3.7)
# thing to talk about -- Mueller report
wordcloud_dfm_3 <- dfm(toks_3, groups = "channel")
textplot_wordcloud(wordcloud_dfm_3, comparison = T,
min_count = 150,
min_size = .5,
max_size = 3.7)
# thing to talk about -- Mueller report
wordcloud_dfm_3 <- dfm(toks_3, groups = "channel")
textplot_wordcloud(wordcloud_dfm_3, comparison = T,
min_count = 200,
min_size = .5,
max_size = 3.7)
?textplot_wordcloud
textplot_wordcloud(wordcloud_dfm_3, comparison = T,
min_count = 200,
min_size = .5,
max_size = 3.7,
width = 1000,
height = 1000)
textplot_wordcloud(wordcloud_dfm_3, comparison = T,
min_count = 200,
min_size = .5,
max_size = 3.7)
key_dfm_2 <- dfm(toks_2, groups = "channel")
key_dfm_2
keyness_2 <- textstat_keyness(key_dfm_2, target = "Channel")
?textstat_keyness
keyness_2 <- textstat_keyness(key_dfm_2, target = "CNNW")
textplot_keyness(keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
CNN_keyness_2 <- textstat_keyness(key_dfm_2, target = "CNNW")
CNN_keyness_2 <- textstat_keyness(key_dfm_2, target = "CNNW")
CNN_key_2 <- textplot_keyness(CNN_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
Fox_keyness_2 <- textstat_keyness(key_dfm_2, target = "FOXNEWSW")
key_dfm_2 <- dfm(toks_2, groups = "channel")
key_dfm_2 <- dfm(toks_2, groups = "channel")
CNN_keyness_2 <- textstat_keyness(key_dfm_2, target = "CNNW")
CNN_key_2 <- textplot_keyness(CNN_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
Fox_keyness_2 <- textstat_keyness(key_dfm_2, target = "FOXNEWSW")
Fox_key_2 <- textplot_keyness(Fox_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
MSNBC_keyness_2 <- textstat_keyness(key_dfm_2, target = "MSNBCW")
MSNBC_key_2 <- textplot_keyness(MSNBC_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
ggarrange(Fox_key_2, MSNBC_key_2)
ggarrange(CNN_key_2, Fox_key_2, MSNBC_key_2)
MSNBC_key_2 <- textplot_keyness(MSNBC_keyness_2,
n = 15L,
margin = .5,
labelsize = 3)
MSNBC_key_2
ggarrange(CNN_key_2, Fox_key_2, MSNBC_key_2)
ggarrange(CNN_key_2, Fox_key_2, MSNBC_key_2,
nrow = 3)
CNN_key_2 <- textplot_keyness(CNN_keyness_2,
n = 15L,
margin = .7,
labelsize = 3)
MSNBC_keyness_2 <- textstat_keyness(key_dfm_2, target = "MSNBCW")
MSNBC_key_2 <- textplot_keyness(MSNBC_keyness_2,
n = 15L,
margin = .7,
labelsize = 3)
ggarrange(CNN_key_2, Fox_key_2, MSNBC_key_2,
nrow = 3)
CNN_key_2
Fox_key_2 <- textplot_keyness(Fox_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
Fox_key_2
Fox_key_2 <- textplot_keyness(Fox_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
MSNBC_keyness_2 <- textstat_keyness(key_dfm_2, target = "MSNBCW")
MSNBC_keyness_2 <- textstat_keyness(key_dfm_2, target = "MSNBCW")
MSNBC_key_2 <- textplot_keyness(MSNBC_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
MSNBC_key_2 <- textplot_keyness(MSNBC_keyness_2,
n = 15L,
margin = .2,
labelsize = 3)
ggarrange(CNN_key_2, Fox_key_2, MSNBC_key_2,
nrow = 3)
key_dfm_3 <- dfm(toks_3, groups = "channel")
# CNN keyness
CNN_keyness_3 <- textstat_keyness(key_dfm_3, target = "CNNW")
CNN_key_3 <- textplot_keyness(CNN_keyness_3,
n = 15L,
margin = .2,
labelsize = 3)
# Fox keyness
Fox_keyness_3 <- textstat_keyness(key_dfm_3, target = "FOXNEWSW")
Fox_key_3 <- textplot_keyness(Fox_keyness_3,
n = 15L,
margin = .2,
labelsize = 3)
# MSNBC keyness
MSNBC_keyness_3 <- textstat_keyness(key_dfm_3, target = "MSNBCW")
MSNBC_key_3 <- textplot_keyness(MSNBC_keyness_3,
n = 15L,
margin = .2,
labelsize = 3)
CNN_key_3
Fox_key_3
MSNBC_key_3
knitr::opts_chunk$set(echo = TRUE)
# Loading necessary packages
library(gt)
library(quanteda)
library(broom)
library(skimr)
library(lubridate)
library(janitor)
library(dotwhisker)
library(tidytext)
library(haven)
library(ggthemes)
library(webshot)
library(stargazer)
library(tidyverse)
library(patchwork)
library(ggrepel)
library(ggpubr)
tidy_chyrons <- read_csv("tidy_chyron_data.csv")
# Turning data into a corpus for quanteda
text_corpus <- corpus(tidy_chyrons, text_field = "text")
summary(tidy_chyrons)
skim(tidy_chyrons)
knitr::opts_chunk$set(echo = TRUE)
# Loading necessary packages
library(gt)
library(quanteda)
library(broom)
library(skimr)
library(lubridate)
library(janitor)
library(dotwhisker)
library(tidytext)
library(haven)
library(ggthemes)
library(webshot)
library(stargazer)
library(tidyverse)
library(patchwork)
library(ggrepel)
library(ggpubr)
tidy_chyrons <- read_csv("tidy_chyron_data.csv")
# Turning data into a corpus for quanteda
text_corpus <- corpus(tidy_chyrons, text_field = "text")
View(tidy_chyrons)
